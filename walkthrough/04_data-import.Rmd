---
title: "Getting your data into R"
author: "Elise Gould"
date: "17/01/2017"
output: github_document
---

```{r "setup", include=FALSE}
require("knitr")
library('tidyverse')
opts_knit$set(root.dir = "../")
```


# Storing data and adding it to your project

*Rectangular, text-files*

We will focus on how to read plain-text rectangular files into R. Non-proprietry formats, such as plain-text files are best used for recording and storing field or other data because they can be used by anyone, without need of special software to view or edit them (British Ecological Society, 2014). Common file extensions include `.txt` or `.CSV` files. CSV stands for Comma Separated Value files. These are simply plain-text files with each line representing a row in the tabular data, and each value being separated by a comma.

One added benefit of using text-files to store data, is that we can put them under version control. Let's do this now.

*1-minute Challenge: add some new data to a project directory*

1. Go to the [workshop repository data folder](https://github.com/egouldo/VicBioCon17_data_wrangling/tree/master/data) and download the csv file called `bat_dat.csv`.
2. Save it to your data folder
3. Commit the file in git and push to GitHub

*A note on creating CSV Files with your own data*

You can transcribe data you have collected into a CSV file using Microsoft Excel or in open-source CSV editors, such as [comma chameleon ](http://comma-chameleon.io). Existing Microsoft Excel spreadsheets can also be converted into `CSV` files. See the [tidy data module](./06_tidy_data.Rmd) for tips on how best to format and structure your data. 

# Importing your data into R

The [`readr` package](http://r4ds.had.co.nz/data-import.html) comes with a selection of tools for importing data into R, turning flat files into R dataframes. For importing other non-CSV flat-file types, check the readr package documentation. The function `readr::read_csv()` reads in CSV files. For those familiar with R, the `read_csv()` function is very similar to base Rs `read.csv()`. Here are some reasons to use `readr` to import your flat-files:

- `readr` functions are about 10x faster than their base equivalents, and you can set a progress bar for importing larger files.
- `readr` functions produce tibbles (more on this below!), and are more careful in how files are imported: character vectors will not be coerced to factors, row names are not used, and column names will not be muddled.
- They are more reproducible: Behaviour of base R functions can be determined by both your operating system and environment variables, so sometimes code that imports data successfully on your computer will not work on someone else's computer.

Lets import the bat data into R using `read_csv()`

```{r}
readr::read_csv("./data/bat_dat.csv")
```

Notice that `readr` prints out the column specification, giving the name and type of each column. This is important for ensuring that the values in a particular column have been read as the right data type. You can manually change the data type if it is read in as the wrong type (see parsing data, below).

## Tweaking `read_csv()`

You can control many different aspects of data import using the many arguments of `read_csv()`.
Often we deal in data collected and given to us by other people, perhaps a consultant or research assistant collected your data. This removes a lot of control over the way the data is formatted. Sometimes people do funny things, like use strange character strings to represent missing values (pro-tip: `NA` is good). If our data comes from a machine in the lab, like a PCR machine log, it might provide metadata or other details about the nature of the collected data. Below are some examples for dealing with these situations.

```{r}
args(read_csv)
```


### Skipping the first line

`read_csv()` uses the first line of data as the data frame's column names. Sometimes you might have metadata in the first few lines, perhaps as a YAML header or as commented lines (using `#`), other times you might be missing column names in your CSV file. 

*1. Skipping metadata or comments with the `skip` argument*

Skipping Comments:

```{r skipping-comments}
readr::read_csv("./data/iris.csv", comment = "#")
```

Skipping a specified number of lines:

```{r skip-lines}
readr::read_csv("./data/iris.csv", skip = 18)
```

*2. Missing Column Names*

Simply set the `col_names` argument to FALSE

```{r missing-column-names}
read_csv("A,2,30\nB,5,60", col_names = FALSE) # '\n' tells read_csv to move onto a new line!
```

Or pass a character vector to `col_names`:
```{r provide-colnames}
read_csv("A,2,30\nB,5,60", 
         col_names = c("site", "native_grass_species", "total_percent_cover")) # c() or concatenate creates a vector
```

### Specifying Missing Values with characters other than NA

```{r}
read_csv("site,native_reptile_species, mean_weight_g\nA,5,50\nB,0,.",
         na = ".")

```

## Parsing column type specification




*1-minute Challenge: load data into R*

1. Return to the script  `01_tidy_data.R` and edit it to import data into your R workspace using readr. Don't forget to assign it to an informatively named object.
2. Source the script to load the data into R's workspace.
3. Commit to git, push to GitHub.








## Overviewing your imported data

Once your data has been imported into R, it's useful to examine it to firstly get a sense of your data, and secondly to aid in diagnosing any inaccuracies that might have occurred either during data capture, data transcription from hard to digital, if your field data was recorded on paper, or perhaps there was an error during the import of the data into R (sometimes R can convert your column to the wrong 'type' e.g. a character instead of numeric value). Overviewing your data after import is simple yet critical task and should take place before any data wrangling or analysis - it could potentially save future-you much time down and heartache down the track.

### QA checks (could go here... OR we could include some specific code to do this after learning our dplyr verbs)

from British Ecological Society (2014) A Guide to Data Management in Ecology and Evolution. (ed K. Harrison). British Ecological Society. [online]. Available from: www.britishecologicalsociety.org/publications/journals.

- ID estimated values, missing values, or double entries
- stat analyses to check for questionable or impossible values and outliers (boxplots could be good for this)
- check format of the data for consistency across the dataset (glimpse, and View can help with this)
- checking the data against similar data to ID potential problems

White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., Supp, S. (2013) Nine simple ways to make it easier to (re)use your data. IEE. 6, 1â€“10.

-  If a column should contain numeric values, check that there are no non-numeric values in the data. 
- Can force check this, using readr, and specifying what col type each should be. it also imputes what each col type should be, and pretty sure gives you a warning when there are inconsistencies. will it coerce it? {} @TODO: use this as an example...
-  Check that empty cells actually represent missing data, and not mistakes in data entry, and indicate that they are empty using the appropriate null values. 
- @TODO {} How to check this... have an exercise on finding missing values.
-  Check for consistency in unit of measurement, data type (e.g., numeric, character), naming scheme (e.g., taxonomy, location), etc.
- correct colnames, missing variables? number of rows, missing observations? `print`ing a `tbl` and `glimpse`, and `View`ing your data object are useful ways to check for these. highlight, WHAT you are looking for. perhaps provide a list of things to look for when using these functions. we could create some mistakes... as we have with the wrong spelling.
- duplicates

*Printing in the console*

Looking at your data frame in the console often clogs your console, and doesn't tell you much about the type of data in each column. The column names might often be obscured. If your console is completely clogged by a dataframe with many rows, you might not even be able to see the first few rows.

**Challenge (easy)**

Run each line of code in the console below.

```{r tbls}
iris # Clogs your console, am I right?
dplyr::tbl_df(iris) # Much better!
```

`tbl_df()` converts your dataframe to the class `tbl` (pronounced 'tibble')

Only the data that can fit on the screen will be displayed. Note that when printed to the console, `tbl`'s dimensions are printed at the top of the print response, and any variables that do not fit onto the screen are listed at the bottom of the printout. Importantly the 'type' of each column is displayed in angle brackets: 

- `<int>` for type integer
- `<dbl>` for doubles, or real numbers
- `<chr>` for character vectors, or strings
- `<dttm>` date-times
- `<lgl>` logical, vectors that contain only `TRUE` or `FALSE`
- `<fctr>` factors, representing categorical variables with a fixed number of possible values
- `<date>` date


In programming language, dplyr functions are said not to have 'side-effects', meaning that unless you assign any of your changes to an object, your code is simply printed the console, rather than being saved to the object in R's memory.


```{r assign_tbls}
class(iris) # Coercion to class tbl has not been saved!
class(dplyr::tbl_df(iris)) # This is the correct output!
iris <- dplyr::tbl_df(iris) # Lets save the changes to R's memory by assigning it to the existing object 'iris'
iris_copy <- dplyr::tbl_df(iris) # if we give a new name, rather than the name of the existing object, this saves the output from our commands to a new object, here its called 'iris_copy'
```

*Getting a glimpse of your data*

Dplyr's `glimpse()` function yields an information dense summary of tbl data, and is particularly handy when you have many variables in your dataset.

```{r glimpse}
dplyr::glimpse(iris)
```

*Viewing your data*

The base utilities `View()` function allows you to navigate your data within RStudio in a spread-sheet like format.You can sort or order your your rows on a particular variable, by hitting the up or down arrow on each column heading. You can also filter rows based on ranges for one or more variables. This functionality is great for building a mental-picture of your data, and for checking on the outputs of your code as you go about wrangling your data. 

**Challenge (easy)**

Run the following line in your console. 

1. In the pop-up window, try sorting your data on one variable
2. Experiment with filtering on one or more rows.

```{r View, eval=FALSE}
View(iris)
```

### interactive vs. scripted work

Note that these functions are primarily used for working interactively. Rather than for in a script.

# @TODO cleaning data (going through the )

File/folder name rules (similar rules for variable names and object names syntax)
- unique
- descriptive
- succinct
- naturall ordered
- consistent
- google protocol / hadleys adapted google protocol.

# References

[R for Data Science: Data Import with readr](http://r4ds.had.co.nz/data-import.html)

British Ecological Society (2014) A Guide to Data Management in Ecology and Evolution. (ed K. Harrison). British Ecological Society. [online]. Available from: www.britishecologicalsociety.org/publications/journals.
