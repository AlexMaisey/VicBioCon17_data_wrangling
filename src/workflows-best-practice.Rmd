---
title: "Putting it all together: Workflows and Best Practice"
author: "Elise Gould"
date: "18/01/2017"
output: github_document
---


File/folder name rules (similar rules for variable names and object names syntax)
- unique
- descriptive
- succinct
- naturall ordered
- consistent
- google protocol / hadleys adapted google protocol.

## WHY DOCUMENT YOUR CODE? and How?
Helps people USE your code.

## What should go in your README?
Again, list from: British Ecological Society (2014) A Guide to Data Management in Ecology and Evolution. (ed K. Harrison). British Ecological Society. [online]. Available from: www.britishecologicalsociety.org/publications/journals.

### project level
project aim, objectives, hypotheses
personnel involved throughout the project, including who to contact for questions
details of sponsors
data structure and organisation of files
software used to prepare and read the data
procedures used for data processing, including quality control and versioning, and the dates these were carried out
known problems limiting data use
instructions on how to cite the data
intellectual property rights and other licensing considerations

Elsewhere, perhaps a documentation folder:
data collection methods, details of instrumentation, environmental conditions during collection, copies of collection instructions if applicable
standards used

### data level
This should not go in the readme.... not sold on a gold standard procedure for this yet

- names, lavels, descriptions for variables
- detailed explanation of codes used
- definitions of acronyms or specialist terminology
- reasons for missing values
- derived data created from the raw file, including the code or algorithm used to create them.
- note that much of this documentaiton is embedded in the analysis

### Metadata

- descriptive: fields such as title, author, abstract keywords
- administrative: rights, permissions and data on formatting
- structural: explanations of e.g. tables in the data
- create metadata AS YOU GO, at the same time... your memory is much worse than you perceive it to be.

White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., Supp, S. (2013) Nine simple ways to make it easier to (re)use your data. IEE. 6, 1–10.

Why metadata: "The first key to using data is understanding it. Meta- data is information about the data, including how it was collected, what the units of measurement are, and descriptions of how to best use the data (Michener and Jones 2012). Clear metadata makes it easier to figure out if a dataset is appropriate for a project. It also makes data easier to use by both the original investigators and by other scientists by making it easy to figure out how to work with the data. Without clear metadata, datasets can be overlooked or go unused due to the difficulty of understanding the data (Fraser and Gluck 1999, Zimmerman 2003). Undocumented data also becomes less useful over time as information about the data is gradually lost (Michener et al. 1997)."

WHAT metadata: 

"Metadata can take several forms, including descript- tive file and column names, a written description of the data, images (i.e., maps, photographs), and specially structured information that can be read by computers (either as separate files or part of the data files; i.e., machine readable metadata). Good metadata should provide the following information (Michener et al. 1997, Zimmerman 2003, Strasser et al. 2012): 
- The what, when, where, and how of data collect- ion.
- How to find and access the data.   
- Suggestions on the suitability of the data for ans-wering specific questions.
- Warnings about known problems or inconsist-
encies in the data, e.g., general descriptions of data limitations or a column in a table to indicate the quality of individual data points.
- Information to check that the data are properly
imported, e.g., the number of rows and columns in the dataset and the total sum of numerical columns."

"Writing good metadata does not necessarily require a lot of extra time. The easiest way to develop metadata is to start describing your data during the planning and data collection stages. This will help you stay organized, make it easier to work with your data after it has been collected, and make eventual publication of the data easier."

# notes on licensing

White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., Supp, S. (2013) Nine simple ways to make it easier to (re)use your data. IEE. 6, 1–10.

"Including an explicit license with your data is the best way to let others know exactly what they can and cannot do with the data you shared. Following the Panton Principles http://pantonprinciples.org we recom- mend:
1. Using well established licenses (or waivers) in order to clearly communicate the rights and resp- onsibilities of both the people providing the data and the people using it.
2. Using the most open license (or waiver) possible, because even minor restrictions on data use can have unintended consequences for the reuse of the data (Schofield et al. 2009, Poisot et al. 2013).
The Creative Commons Zero (CC0) public domain dedication places no restrictions on data use and is considered by many to be one of the best ways to share data (e.g., (Schofield et al. 2009, Poisot et al. 2013), http://blog.datadryad.org/2011/10/05/why-does-dryad- use-cc0/). Several other licenses and waivers also accomplish these same goals http://opendefinition.org/ licenses/#Data. Having a clear and open license (or waiver) will increase the chance that other scientists will be comfortable using your data."

### Setting up your project directory

What folders should you have, what files should go where

@TODO find my notes on this

Noble, W. S. (2009) A Quick Guide to Organizing Computational Biology Projects (ed F. Lewitter). PLoS Comp Biol. 5, e1000424.

### File paths, relative vs. hard-coded

### RAW data
DATA MASTERFILE--- DO NOT EDIT OVER MISTAKES.... want the data as RAW as possible. WHY find refs on this.



Provide RAW and PRocessed forms of the data.
Explain the differences between them with metadata, and commented code.
SHARE the code that morphs the code from raw to processed for analysis.

"Often, the data used in scientific analyses are mod- ified in some way from the original form in which they were collected. Values are averaged, units are convert- ed, or indices are calculated from direct measurements or observations to address the focal research questions and to fix issues associated with the raw data. However, the best way to process data depends on the question being asked, and corrections for common data limit- ations often change as better approaches are developed. It can also be very difficult to combine data from mult- iple sources that have each been processed in different ways. Therefore, to make your data as useful as possible it is best to share the data in as raw a form as possible. That means providing your data in a form that is as close as possible to the field measurements and observ- ations from which your analysis started.
This is not to say that your data are best suited for analysis in the raw form, but providing it in the raw form gives data users the most flexibility."

### R scripts, vs. Rmarkdown documents

when should something go into a script, and when should it go into an RMarkdown document...
tricky topic, and a decision youll want to make depending on your own personal workflow.

My rules:

- R scripts for cleaning and tidying data, generating data files, or long analyses.
- RMarkdown files for exploratory data analyses, WHAT ELSE.
- generating or altering of files should happen NOT in RMarkdown docs.

- put as MUCH as you can in R. All changes should occurr here. NOT in excel.

# tidy data for ecology

each row is a single observation, each column a single variable or type of measurement.
One column for each type of informaiton. EXCEPT, cross-tab structure data, where columns contain measurements of the same variable (e.g. in differen sites, treatments). Nah, should keep it long I reckon.
single value per cell (i.e. no units in the cell) You can put the units into a metadata file.
break taxonomic information up into single components, with one column each for family, genus, species, subspecies, etc. Helps having to split strings later, BUT, is do-able with dplyr.

# Metadata for ecology:

### Contextual data

"Ecological and evolutionary data are often combined with other kinds of data. You can make it easier to com- bine your data with other data sources by including con- textual data that appears across similar data sources. Two of the most common kinds of contextual data in ecology and evolution are taxonomy and geographic location. While this type of data is known and recorded in most studies (e.g, in field notebooks, on maps) it is frequently not included with the data. In general, if you have collected additional data or notes about a study organism or field site, there is a good chance that it will be useful to someone else, so including it with your data when you share it is a good idea. This kind of informat- ion can be included either as part of the data itself (e.g., in a new column or an additional table) or can be includ- ed in the metadata (e.g., the geographic location of the study site). For geographic data it is also important to include the datum (e.g., WGS-84) and sufficient précis- ion (e.g., 4 decimals places if using decimal degrees) to allow the data to be combined with other geographic datasets." White, E., Baldridge, E., Brym, Z., Locey, K., McGlinn, D., Supp, S. (2013) Nine simple ways to make it easier to (re)use your data. IEE. 6, 1–10.

## Metadata
describe the project, file contents, location, date. Dont add a version, because this will be covered by version control system. Not sold on adding initials to the file name, because i think good metadata practice can cover this.
2

issue: column headers are values, not variables names

pp 5 - 6:
Often datasets are designed for presentation, rather than for analysis, and so column headers are actually values not variables names.
Sometimes we don't actually want tidy data. In some cases it can be useful in messy form. It can be stored efficiently for, say completely crossed designs, and can be useful for certain matrix operations.
Data might not be 'tidy' when we transcribe it from our field data sheets to digital spreadsheets / csv's because its tedious to repeatedly type some variables out. 

what do we do when we want our data tidy, but its easier to capture it messy / wide?

### When NOT to use tidy data:

Reasons when to use other non-tidy data strucutres (from R4ds):

1. alternative representations have substantial performance or space advantages
2. specialised fields have evolved their own conventions for storing data that may be quite different to the conventions of tidy data.
*A note on R's memory, scripts and the console* @TODO COMPLETE THIS

workspace vs. scripts. what's a workspace

R's memory is usually reset at the end of each session, or at least it should be (i.e. each time you quit the project or close RStudio). Although you can opt to save the workspace at the end of the session automatically (it is written as an `.RData` file). This is a sure way to encounter errors, although it can be convenient. Errors can be encounted if you have modified the code generating an R object.


in RStudio you can opt to load any objects that you had in Rs memory at time of closing. But this is a sure way to encounter errors, especially if you have edited your code in an editor, but havent sent those commands to the console to update the object. It is best practice to NOT save the 'workspace', or all of the in-memory objects, at the 

Ensure you have the following settings in-place for RStudio:

```
RStudio>Preferences/Settings > General:
Restore .RData into workspace on start-up: de-select if selected
Save workspace to .RData on exit: -> NEVER
```
